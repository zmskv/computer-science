
# wget - утилита для зеркалирования сайтов

Простая консольная утилита, написанная на Go, для рекурсивного скачивания веб-сайтов и их сохранения для просмотра в офлайн-режиме. Аналог `wget -m`.

---

## Описание

Эта утилита принимает URL-адрес и глубину рекурсии в качестве входных данных. Она начинает скачивание с указанной страницы, парсит HTML для поиска ссылок на другие страницы, стили (CSS), скрипты (JS) и изображения. Затем она рекурсивно скачивает все найденные ресурсы, которые принадлежат тому же домену.

## Возможности

-   **Рекурсивная загрузка:** Скачивает не только указанную страницу, но и страницы, на которые она ссылается.
-   **Контроль глубины:** Позволяет ограничить глубину обхода с помощью флага `-depth`.
-   **Загрузка всех ресурсов:** Скачивает HTML, CSS, JavaScript, изображения и другие типы файлов.
-   **Перезапись ссылок для офлайн-просмотра:** Автоматически преобразует все ссылки (`href`, `src`) в относительные пути для корректной работы локальной копии.
-   **Параллельная загрузка:** Использует пул воркеров для одновременного скачивания нескольких ресурсов, что значительно ускоряет процесс.
-   **Предотвращение дубликатов:** Отслеживает посещенные URL, чтобы избежать повторного скачивания одного и того же ресурса и зацикливания.
-   **Корректная структура каталогов:** Воссоздает структуру сайта в локальной файловой системе.

## Требования

-   Go версии 1.18 или выше.

```sh
./wget -url=<целевой_URL> [-depth=<глубина_рекурсии>]
```

-   `-url` (обязательный): URL сайта, с которого нужно начать зеркалирование.
-   `-depth` (опциональный): Глубина рекурсивного обхода. По умолчанию `1`.

### Пример

```sh
./wget -url="http://example.com" -depth=2
```

После выполнения команды в текущей директории будет создана папка с именем хоста (например, `example.com`), содержащая зеркальную копию сайта.

## Структура проекта

Проект имеет модульную структуру для лучшего разделения ответственности:

-   `main.go`: Точка входа. Отвечает за парсинг флагов командной строки и инициализацию компонентов.
-   `/internal/crawler`: Основной оркестратор. Управляет очередью задач, пулом воркеров и отслеживает посещенные URL.
-   `/internal/downloader`: Отвечает за выполнение HTTP-запросов и скачивание контента по URL.
-   `/internal/parser`: Выполняет две ключевые задачи:
    1.  Извлечение ссылок из HTML-документов.
    2.  Перезапись ссылок в HTML для офлайн-просмотра.
-   `/internal/saver`: Отвечает за сохранение скачанных файлов на диск и преобразование URL в корректные локальные пути.
